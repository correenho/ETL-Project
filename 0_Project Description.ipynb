{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Project Description\n",
    "\n",
    "## I. Objective\n",
    "\n",
    "This project aims to set up the data structure for a database that stores information from multiple sources about movies released in 2017. The reason for picking the year 2017 is that it's the latest year where we can find data from all defined resources. And it's limited to 1 year mostly due to the time limit to scrape information off websites e.g. Rotten Tomatoes.\n",
    "\n",
    "## II. Data Collection Approach and Sources\n",
    "\n",
    "### 1. Identify the topic, objective, project scope and expected outcome (Jeremy & Correen)\n",
    "We decided to set up a database about movies to include the information below for future potential analyses.\n",
    "1) Basic information including title, studio, director, cast, release year, budget, cost, boxrevenue, content rating, etc.\n",
    "2) Viewer ratings from both IMDB (IMDB rating) and Rotten Tomatoes (Rotten Tomatoes Meter).\n",
    "3) Professional Critic scores from IMDB (IMDB Metascore).\n",
    "\n",
    "### 2. Identify the movie lists (starting point) with data clean up\n",
    "- Although there are some csv datasets from Data.world and Kaggle that could serve the purpose, we found that their data are out of date. So we found another datasource (link: https://www.boxofficemojo.com/year/2017/?grossesOption=totalGrosses&sort=openingWeekendGross&ref_=bo_yld__resort#table) as the starting point for the movie list. \n",
    "- Data cleanup (Jeremy) done to set up the dataframe, modify data formats, drop duplicate or excessive columns, and most importantly isolate the release Year information for our next steps.\n",
    "\n",
    "### 3. Update movie information from OMDBAPI.com\n",
    "- Leveraged the API to update the basic information about each movie to replace the outdated information in the original csv file, added data such as number of theaters each movie reached and DVD release dates. (Jeremy)\n",
    "- Leveraged the API to update the Metascores (professional critics) that were missing in the original csv file. (Correen)\n",
    "\n",
    "### 4. Scrape the Rotten Tomatoes Meter with Splinter from rottentomatoes.com (Correen)\n",
    "- Extracted the movie list released in 2017 from the master dataframe after cleanup. \n",
    "- Chopped up the list into groups of 200 - 250 each for scraping.\n",
    "- Captured the scraped results, cleaned up the data format to float instead of string to allow data analyses. \n",
    "\n",
    "### 5. Feed data back into master dataset and set up the SQL Schema (Jeremy)\n",
    "- Merged the new data back into the master dataframe.\n",
    "- For nan results in Rotten Tomatoes Meter, replace them with the mean value of the dataset to allow future data analyses.\n",
    "- Set up the SQL database schema and link to the table as a complete database.\n",
    "\n",
    "### 6. Sample query (both)\n",
    "- \n",
    "\n",
    "\n",
    "## III. Key Challenges and Solutions\n",
    "### 1. Identify the right SQL database structure. \n",
    "At first we wanted to set up separate tables for basic info, cast, budget/income, IMDB ratings, and Rotten Tomatoes respectively. However, after talking to TA, it seems that there is no point in separating the tables as it doesn't increase storage or operation efficiency. So we decided to combine everything into 1 table for operation efficiency.\n",
    "### 2. Extract the correct search results.\n",
    "While searching for ratings on Rotten Tomatoes, since the movie names may have the same keywords as our movie to search for, we decided to use exact work match plus release year matching to increase accuracy. This resultsed in some loss of data because the movie names we found from original source maybe have a suffix in a foreign languague, or the release year information might be wrong. But these are minor impacts based on random sample investigation. \n",
    "### 3. Scraping efficiency.\n",
    "While scraping, the server has a mechanism to protect itself and hence we got 'Access Denied' multiple times. Therefore, we had to chop up the movie lists into subsets of 200-250, and build in a sleep timer of 5 - 10 seconds to wait for the search results to be loaded. Ths resulted in the fact that we had to limit to only one year (2017) instead of the original plan to scrape for all the years since 2000.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
